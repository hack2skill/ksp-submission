{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2281b484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6893f66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f745d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def load_data(dataset_path):\n",
    "  \n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    classes = os.listdir(dataset_path)\n",
    "    for i, c in enumerate(classes):\n",
    "        class_path = os.path.join(dataset_path, c)\n",
    "        image_path = os.path.join(class_path, os.listdir(class_path)[0])\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image,(224,224))\n",
    "        images.append(image)\n",
    "        labels.append(i)\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n",
    "def preprocess_data(images, labels):\n",
    " \n",
    "    images = images / 255.0\n",
    "\n",
    "    labels = tf.keras.utils.to_categorical(labels, num_classes=488)\n",
    "\n",
    "    \n",
    "    indices = np.arange(images.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    images = images[indices]\n",
    "    labels = labels[indices]\n",
    "    split = int(0.8 * images.shape[0])\n",
    "    train_images = images\n",
    "    test_images = images[split:]\n",
    "    train_labels = labels\n",
    "    test_labels = labels[split:]\n",
    "\n",
    " \n",
    "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    train_datagen.fit(train_images)\n",
    "    return train_images, train_labels, test_images, test_labels, train_datagen\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b46c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_data('datas/')\n",
    "  \n",
    "\n",
    "train_images, train_labels, test_images, test_labels, train_datagen = preprocess_data(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec77761c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96590519",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = tf.keras.applications.mobilenet.preprocess_input(images)\n",
    "\n",
    "base_model = tf.keras.applications.MobileNetV2(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(488, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(train_images,train_labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657024b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892fd5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3f5526",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = train_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82761bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = cv2.imread(\"missing_FIR _images/image_0_1530.jpeg\")\n",
    "image=tf.image.resize(test_img,(224,224))\n",
    "image = np.expand_dims(image, axis=0)\n",
    "image_p=image/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c501af7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.expand_dims(img, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ece6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8e1456",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.expand_dims(f, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80936426",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41de35ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = model.predict(image_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7152cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aecefc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b32f7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_excel(\"Sample Missing Persons FIRS.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e9b7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fff= train_df.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dc75c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Photo_Full_front\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f4b8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_df.where(train_df['Photo_Full_front']==\"Scan_20230#_ae729682-d77d-4c78-a213-3b84bfff06e21259.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46781c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = a.dropna().to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d56681",
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ee7dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ing = train_df[\"Photo_Full_front\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6ab648",
   "metadata": {},
   "outputs": [],
   "source": [
    "ing[pre.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f0ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale =1/255.,\n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "\n",
    "data_gen.fit(images)\n",
    "\n",
    "\n",
    "images_augmented, labels_augmented = next(data_gen.flow(images, labels, batch_size=len(images)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ea69e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324b85f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca1087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img= cv2.imread(\"datas/0009#_33fee390-f612-4acc-8dc3-b4e6bb61f8b81172_page-0001/0009#_33fee390-f612-4acc-8dc3-b4e6bb61f8b81172_page-0001.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0719222",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b606d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9358928f190410cf529a417785756cd1d764b75d3fb661dab4eb897a1b82284"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
