{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "from django.http import HttpResponse\n",
    "from django.template import loader\n",
    "from django.core.mail import send_mail\n",
    "from django.shortcuts import render\n",
    "\n",
    "from google_play_scraper import app\n",
    "from google_play_scraper import search\n",
    "from google_play_scraper import permissions\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import parse_qs\n",
    "\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import unidecode\n",
    "from string import digits\n",
    "import requests\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import hstack\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('apps.txt') as f:\n",
    "    apps = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = []\n",
    "description = []\n",
    "score = []\n",
    "rating = []\n",
    "reviews = []\n",
    "email = []\n",
    "domain = []\n",
    "\n",
    "contacts = []\n",
    "location = []\n",
    "storage = []\n",
    "microphone = []\n",
    "sms = []\n",
    "wifi = []\n",
    "deviceid = []\n",
    "photos = []\n",
    "phone = []\n",
    "camera = []\n",
    "\n",
    "other = []\n",
    "\n",
    "for id in apps:\n",
    "    result = app(\n",
    "        id,\n",
    "        lang='en',  # parse_qs(parsed_url.query)['hl'][0],\n",
    "        country='in'  # parse_qs(parsed_url.query)['gl'][0]\n",
    "    )\n",
    "    title.append(result['title'])\n",
    "    description.append(result['description'])\n",
    "    rating.append(result['ratings'])\n",
    "    em = result['developerEmail']\n",
    "    domain.append(em[em.index('@') + 1:])\n",
    "    email.append(em[0: em.index('@')])\n",
    "    \n",
    "    rr = result['comments']\n",
    "    rr_string = ' '.join(word for word in rr)\n",
    "    reviews.append(rr_string)\n",
    "    score.append(result['score'])\n",
    "    permit = permissions(\n",
    "        id,\n",
    "        lang='en',  # defaults to 'en'\n",
    "        country='in',  # defaults to 'us'\n",
    "    )  # dictionary\n",
    "    permisions_list = list(permit.keys())\n",
    "\n",
    "    if (\"Location\" in permisions_list):\n",
    "        location.append(1)\n",
    "    else:\n",
    "        location.append(0)\n",
    "\n",
    "    if (\"Storage\" in permisions_list):\n",
    "        storage.append(1)\n",
    "    else:\n",
    "        storage.append(0)\n",
    "\n",
    "    if (\"Contacts\" in permisions_list):\n",
    "        contacts.append(1)\n",
    "    else:\n",
    "        contacts.append(0)\n",
    "\n",
    "    if (\"SMS\" in permisions_list):\n",
    "        sms.append(1)\n",
    "    else:\n",
    "        sms.append(0)\n",
    "    \n",
    "    if (\"Microphone\" in permisions_list):\n",
    "        microphone.append(1)\n",
    "    else:\n",
    "        microphone.append(0)\n",
    "\n",
    "    if (\"Phone\" in permisions_list):\n",
    "        phone.append(1)\n",
    "    else:\n",
    "        phone.append(0)\n",
    "\n",
    "    if (\"Wi-Fi connection information\" in permisions_list):\n",
    "        wifi.append(1)\n",
    "    else:\n",
    "        wifi.append(0)\n",
    "\n",
    "    if (\"Camera\" in permisions_list):\n",
    "        camera.append(1)\n",
    "    else:\n",
    "        camera.append(0)\n",
    "\n",
    "    if (\"Photos/Media/Files\" in permisions_list):\n",
    "        photos.append(1)\n",
    "    else:\n",
    "        photos.append(0)\n",
    "\n",
    "    if (\"Device ID & call information\" in permisions_list):\n",
    "        deviceid.append(1)\n",
    "    else:\n",
    "        deviceid.append(0)\n",
    "    a1 = []\n",
    "    if('Other' in permisions_list):\n",
    "        a1 = permit['Other']\n",
    "    a2 = []\n",
    "    if('Uncategorized' in permisions_list):\n",
    "        a2 = permit['Uncategorized']\n",
    "    a3 = a1 + a2\n",
    "    a3_string = ' '.join(word for word in a3) \n",
    "    other.append(a3_string)\n",
    "\n",
    "d = {'title': title, 'description': description, 'score': score, 'rating': rating, 'review': reviews, 'email': email, 'domain': domain, 'contacts': contacts, 'location': location, 'storage': storage, 'microphone': microphone, 'sms': sms, 'wifi': wifi, 'deviceid': deviceid, 'photos': photos, 'phone': phone, 'camera': camera, 'other': other}\n",
    "\n",
    "df = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"myfile.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"myfile.csv\")\n",
    "\n",
    "train1 = df['description']\n",
    "train2 = df['review']\n",
    "train3 = df['other']\n",
    "train4 = df['domain']\n",
    "train5 = df['email']\n",
    "\n",
    "k1 = len(train5)\n",
    "k1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2), min_df=1, max_df=1,\n",
    "    max_features=200\n",
    ")\n",
    "\n",
    "word_vectorizer.fit(train1)\n",
    "f1 = word_vectorizer.transform(train1)\n",
    "word_vectorizer.fit(train2)\n",
    "f2 = word_vectorizer.transform(train2)\n",
    "word_vectorizer.fit(train3)\n",
    "f3 = word_vectorizer.transform(train3)\n",
    "word_vectorizer.fit(train4)\n",
    "f4 = word_vectorizer.transform(train4)\n",
    "word_vectorizer.fit(train5)\n",
    "f5 = word_vectorizer.transform(train5)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    stop_words='english',\n",
    "    ngram_range=(2, 6), min_df=1, max_df=1,\n",
    "    max_features=200\n",
    ")\n",
    "\n",
    "char_vectorizer.fit(train1)\n",
    "c1 = char_vectorizer.transform(train1)\n",
    "char_vectorizer.fit(train2)\n",
    "c2 = char_vectorizer.transform(train2)\n",
    "char_vectorizer.fit(train3)\n",
    "c3 = char_vectorizer.transform(train3)\n",
    "char_vectorizer.fit(train4)\n",
    "c4 = char_vectorizer.transform(train4)\n",
    "char_vectorizer.fit(train5)\n",
    "c5 = char_vectorizer.transform(train5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 1572)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = np.array(df['score']).reshape(k1,1)\n",
    "a2 = np.array(df['rating']).reshape(k1,1)\n",
    "a3 = np.array(df['location']).reshape(k1,1)\n",
    "a4 = np.array(df['storage']).reshape(k1,1)\n",
    "a5 = np.array(df['microphone']).reshape(k1,1)\n",
    "a6 = np.array(df['sms']).reshape(k1,1)\n",
    "a7 = np.array(df['wifi']).reshape(k1,1)\n",
    "a8 = np.array(df['deviceid']).reshape(k1,1)\n",
    "a9 = np.array(df['photos']).reshape(k1,1)\n",
    "a10 = np.array(df['phone']).reshape(k1,1)\n",
    "a11 = np.array(df['camera']).reshape(k1,1)\n",
    "a12 = np.array(df['contacts']).reshape(k1,1)\n",
    "\n",
    "train_features = hstack([f1,f2,f3,f4,f5,c1,c2,c3,c4,c5,a1,a2,a3,a4,a5,a6,a7,a8,a9,a10,a11,a12])\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.pipeline import make_union\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.ensemble import BaggingClassifier\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import Ridge\n",
    "from itertools import product\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "# from sklearn.naive_bayes import ComplementNB\n",
    "from scipy.sparse import csr_matrix\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "import regex as re\n",
    "import re as r\n",
    "import regex, string\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "scores = []\n",
    "ridge_file = 'ridge.pckl'\n",
    "ridge_model_pkl = open(ridge_file, 'wb')\n",
    "train_target = df['fraud']\n",
    "\n",
    "classifier = LogisticRegression(random_state=0)\n",
    "# classifier = Ridge(alpha=29, copy_X=True, fit_intercept=True, solver='sag',\n",
    "#                    max_iter=150, random_state=0,  tol=0.0025)\n",
    "\n",
    "# cv_score = np.mean(cross_val_score(classifier, train_features, train_target, cv=3,\n",
    "#                                    scoring='roc_auc'))\n",
    "# scores.append(cv_score)\n",
    "\n",
    "l = EasyEnsembleClassifier(base_estimator=LogisticRegression(C=2, solver='sag', max_iter=500))\n",
    "n = EasyEnsembleClassifier(base_estimator=SGDClassifier(alpha=.0002, max_iter=180, penalty=\"l2\", loss='modified_huber'))\n",
    "o = LogisticRegression(C=2, dual=True, max_iter=500)\n",
    "p = RandomForestClassifier(criterion='gini',\n",
    "    max_depth=100, max_features=1000, max_leaf_nodes=None, min_samples_split=10,\n",
    "    min_weight_fraction_leaf=0.0, n_estimators=120)  \n",
    "#     r = Ridge(alpha=29, copy_X=True, fit_intercept=True, solver='sag',\n",
    "#                      max_iter=500,   normalize=False, random_state=0,  tol=0.0025)\n",
    "classifier.fit(train_features, train_target)\n",
    "pickle.dump(classifier, ridge_model_pkl)\n",
    "\n",
    "ridge_model_pkl.close()\n",
    "# print('Total CV score is {}'.format(np.mean(scores)))\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1177)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('myfile2.csv')\n",
    "\n",
    "test1 = df2['description']\n",
    "test2 = df2['review']\n",
    "test3 = df2['other']\n",
    "test4 = df2['domain']\n",
    "test5 = df2['email']\n",
    "\n",
    "k = len(test1)\n",
    "\n",
    "word_vectorizer.fit(test1)\n",
    "ft1 = word_vectorizer.transform(test1)\n",
    "word_vectorizer.fit(test2)\n",
    "ft2 = word_vectorizer.transform(test2)\n",
    "word_vectorizer.fit(test3)\n",
    "ft3 = word_vectorizer.transform(test3)\n",
    "word_vectorizer.fit(test4)\n",
    "ft4 = word_vectorizer.transform(test4)\n",
    "word_vectorizer.fit(test5)\n",
    "ft5 = word_vectorizer.transform(test5)\n",
    "\n",
    "\n",
    "char_vectorizer.fit(test1)\n",
    "ct1 = char_vectorizer.transform(test1)\n",
    "char_vectorizer.fit(test2)\n",
    "ct2 = char_vectorizer.transform(test2)\n",
    "char_vectorizer.fit(test3)\n",
    "ct3 = char_vectorizer.transform(test3)\n",
    "char_vectorizer.fit(test4)\n",
    "ct4 = char_vectorizer.transform(test4)\n",
    "char_vectorizer.fit(test5)\n",
    "ct5 = char_vectorizer.transform(test5)\n",
    "\n",
    "at1 = np.array(df2['score']).reshape(k, 1)\n",
    "at2 = np.array(df2['rating']).reshape(k, 1)\n",
    "at3 = np.array(df2['location']).reshape(k, 1)\n",
    "at4 = np.array(df2['storage']).reshape(k, 1)\n",
    "at5 = np.array(df2['microphone']).reshape(k, 1)\n",
    "at6 = np.array(df2['sms']).reshape(k, 1)\n",
    "at7 = np.array(df2['wifi']).reshape(k, 1)\n",
    "at8 = np.array(df2['deviceid']).reshape(k, 1)\n",
    "at9 = np.array(df2['photos']).reshape(k, 1)\n",
    "at10 = np.array(df2['phone']).reshape(k, 1)\n",
    "at11 = np.array(df2['camera']).reshape(k, 1)\n",
    "at12 = np.array(df2['contacts']).reshape(k, 1)\n",
    "\n",
    "test_features = hstack([ft1, ft2, ft3, ft4, ft5, ct1, ct2, ct3, ct4, ct5, at1, at2, at3, at4, at5, at6, at7, at8, at9, at10, at11, at12])\n",
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72747889, 0.27252111])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = classifier.predict_proba(train_features)\n",
    "m[0]\n",
    "m[50]\n",
    "m[44]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
